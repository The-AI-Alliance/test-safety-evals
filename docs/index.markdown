---
layout: default
title: Home
nav_order: 10
has_children: false
---

# Trust and Safety Evaluations Initiative

| **Authors** | [The AI Alliance Trust and Safety Work Group](https://thealliance.ai/focus-areas/trust-and-safety){:target="ai-alliance-tns"} |
| **Last Update**  | V0.3.1, 2024-12-12 |

Welcome to the **The AI Alliance** initiative for **Trust and Safety Evaluations**.

> **Tips:** 
>
> 1. Use the search box at the top of this page to find specific content.
> 2. Capitalized, italicized terms link to a [glossary of terms]({{site.baseurl}}/glossary).

Much like other software, generative AI (&ldquo;GenAI&rdquo;) [_Models_]({{site.baseurl}}/glossary/#model) and the [_AI Systems_]({{site.baseurl}}/glossary/#ai-system) that use them need to be trusted and useful to their users.

[_Evaluation_]({{site.baseurl}}/glossary/#evaluation) aims to provide the evidence for gaining usersâ€™ trust in models and systems. More specifically, evaluation refers to the capability of measuring and quantifying how a model or system responds to inputs. Are the responses within acceptable bounds, for example free of hate speech and [_Hallucinations_]({{site.baseurl}}/glossary/#hallucination), are they useful to users, cost-effective, etc.?

There are many organizations working on evaluations for safety, broadly defined, and other kinds of measurements, as well as [_Benchmarks_]({{site.baseurl}}/glossary/#benchmark) that aggregate some evaluations and [_Leaderboards_]({{site.baseurl}}/glossary/#leaderboard) that let you see how some models and systems do against benchmarks, without having to execute these benchmarks yourself. 

The **Trust and Safety Evaluations Initiative** addresses several under-served needs:

1. While very good [Taxonomies]({{site.baseurl}}/glossary/#taxonomy) of evaluation in the areas of risk and harms have [emerged]({{site.baseurl}}/taxonomy/taxonomy/#why-build-a-taxonomy), there are other areas of interest where a standard taxonomy, with corresponding evaluations, would be useful. See [taxonomy]({{site.baseurl}}/taxonomy/taxonomy).
2. [_Evaluators_]({{site.baseurl}}/glossary/#evaluator) that implement evaluations in the taxonomy are needed. Some areas are well-covered, while others have no available evaluators. These evaluators can be aggregated into benchmarks. See [evaluators]({{site.baseurl}}/evaluators/evaluators).
3. Leaderboards are needed that provide unique, user-configurable views on different benchmark combinations, which help users focus on the benchmarks most relevant to their needs. See [leaderboards]({{site.baseurl}}/leaderboards/leaderboards).
4. Users need a _reference stack_ of industry-standard OSS tools for evaluation, especially at [_Inference_]({{site.baseurl}}/glossary/#inference) time. See [Evaluation Platform Reference Stack]({{site.baseurl}}/ref-stack/ref-stack).

This website provides the documentation for this initiative, with links to other resources, including code and leaderboards, as they become available.

Are you interested in contributing? If so, please see the [contributing]({{site.baseurl}}/contributing) page for information on how you can get involved.

This site is organized into the following sections:

* [Glossary of Terms]({{site.baseurl}}/glossary)
* [User Personae and Their Needs]({{site.baseurl}}/user-personae/user-personae)
* [Taxonomy]({{site.baseurl}}/taxonomy/taxonomy)
* [Evaluators]({{site.baseurl}}/evaluators/evaluators)
* [Leaderboards]({{site.baseurl}}/leaderboards/leaderboards)
* [Evaluation Platform Reference Stack]({{site.baseurl}}/ref-stack/ref-stack)

Additional links:

* [Contributing]({{site.baseurl}}/contributing): We welcome your contributions! Here's how you can contribute.
* [About Us]({{site.baseurl}}/about): More about the AI Alliance and this initiative.
* [GitHub Repo](https://github.com/The-AI-Alliance/trust-safety-evals){:target="repo"}
* [The AI Alliance](https://thealliance.ai){:target="ai-alliance"}: The AI Alliance website.

### Version History


| Version  | Date       |
| :------- | :--------- |
| V0.3.1   | 2024-12-12 |
| V0.3.0   | 2024-12-05 |
| V0.2.0   | 2024-11-15 |
| V0.1.0   | 2024-10-12 |
